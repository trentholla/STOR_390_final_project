---
title: 'EDA: Initial Results'
author: 'STOR 390: Group 5'
output: html_document
---

# --------------------------------------------------------------------

```{r setup, include=F}
library(tidyverse)
library(scales)
library(car)
```

###The Data

The two data sets, one from a Math class and the other from a Portuguese Class, are imported and merged. Observations with a 0 as the final grade (G3) were dropped, as the other variables did not support this outcome. We assume there were other factors that led to the 0 final scores (e.g. the student did not take the assessment). We also removed the observation with the sole final grade of 1 as an outlier (see grouped summary table below). 

```{r data}
# Read in data
math <- read.table("student-mat.csv", sep=";", header=TRUE)
port <- read.table("student-por.csv", sep=";", header=TRUE)


# Merge data to look at general student aptitude rather than separate math/Portuguese scores 
math <- mutate(math, class = "M")
port <- mutate(port, class = "P")

data <- rbind(math, port)


# Check final score distributions
group_by(data, G3) %>%
    summarize(cnt = n())

# Remove observations with 0 or 1 as G3 score
data <- data[data$G3 != 0,]
data <- data[data$G3 != 1,]
```

```{r factorization, include=F}
data <- mutate(data, studytime = factor(studytime),
               health = factor(health))
```

# --------------------------------------------------------------------

### Factors Affecting Final Grades


Is our intutition that students who study more perform better correct? Weekly study time is divided into 4 groups: where "1"" is less than 2 hours and "4"" is greater than 10. We see in the plot below that on average, final grades increase as study time increases.
```{r study, echo=F}
ggplot(data, aes(x = studytime, y = G3))+ 
    geom_boxplot() + 
    labs(x = 'Weekly Study Time', y = 'Final Grade', title = 'Studying Improves Final Grades') +
    theme_classic()
```

The following graph has an interesting result, as the healthiest group of students (5) have one of the lowest average grades (which is somewhat counterintuitive). However, the difference in average grades in by less than 1 point among all health levels. 
```{r health, echo=F}
ggplot(data, aes(x = health, y = G3)) +  
    geom_boxplot() + 
    labs(x = 'Health Status', y = 'Final Grade', title = 'Final Grade Not Impacted by Health') +
    theme_classic()
```

# --------------------------------------------------------------------

### Other Interesting Relationships

We can see below that a higher percentage of the students at Gabriel Pereira chose that school for it's reputation. It makes sense that Gabriel Pereira would have a better reputation, since it has higher scores in both Math and Portuguese. 
```{r reason, echo=F}
ggplot(data, aes(x = school, fill = reason)) + 
    geom_bar(position = "fill") + 
    scale_y_continuous(labels = percent) + 
    scale_x_discrete(labels = c("Gabriel Pereira" , "Mousinho da Silveira")) +
    labs(x = "School", y = "Percent of Student Population", title = "Higher percentage of students chose Gabriel Pereira for it's reputation") +
    theme_classic()

ggplot(data, aes(x = school, y = G3, color = class)) + 
    geom_boxplot() +
    scale_x_discrete(labels = c("Gabriel Pereira" , "Mousinho da Silveira")) +
    scale_color_discrete(name = "Type of Class", labels = c("Math", "Portuguese")) +
    labs(x = "School", y = "Final Grade", title = "Gabriel Pereira students have higher average scores in both math and Portuguese") + 
    theme_classic()


```

<<<<<<< HEAD
As for the initial model, a simple linear regression has a high R^2 value and is a good starting place for modelling. This was accomplished by splitting the data into a training and test set. The models were created using the training set and tested on the test set in order to prevent overfitting. The predicted values given by the models come close to the actual ones, while the test errors are only slightly greater than the training errors. Another interesting find is that most of the variables were not statistically significant in the regression.
```{r}
#training and test sets ------------------------------------------
=======




# --------------------------------------------------------------------

### Preliminary Models

As for the initial model, a simple linear regression has a high R^2 value and is a good starting place for modelling. The predicted values come close to the actual ones. (Calculate test error tomorrow).
```{r linear}
>>>>>>> fa048a92bbc5ab01f420516e354c93d7a818375e
n <- dim(data)[1]

# number of observations that go in the training st
n_tr <- floor(n * .8)


# randomly select n_tr numbers, without replacement, from 1...n
tr_indices <- sample(x=1:n, size=n_tr, replace=FALSE)

# break the data into a non-overlapping train and test set
train <- data[tr_indices, ]
test <- data[-tr_indices, ]


#comprehensive linear model ----------------------------------- 
woah <- lm(G3 ~ school + sex + age + address + famsize + Pstatus + Medu + Fedu + Mjob + Fjob+ reason + guardian + traveltime + studytime + failures + schoolsup + famsup + paid + activities + nursery + higher + internet + romantic + famrel + freetime + goout + Dalc + Walc + health + absences + G1 + G2 + class, train)

summary(woah)
vif(woah)

#other models -------------------------------------------
model2 <- lm(G3 ~ age + schoolsup + paid + absences + G1 + G2 + class, train)
summary(model2)
MSE2 <- mean(model2$residuals^2)

model3 <- lm(G3 ~ G1 + G2 + class, train, na.action = na.omit)
summary(model3)
MSE3 <- mean(model3$residuals^2)

model4 <- lm(G3 ~ age + schoolsup + paid + absences + G1 + G2 +I(G1^2) + I(G2^2) + class, train)
summary(model4)
MSE4 <- mean(model4$residuals^2)

model5 <- lm(G3 ~ absences + G1 + G2 + I(G1*G2) + class, train)
summary(model5)
MSE5 <- mean(model5$residuals^2)

models <- list(model1, model2, model3, model4, model5)

#error --------------------------------------------------
error <- tibble(model_numberr=c(1,2,3,4,5),
                MSE_tr=c(MSE1, MSE2, MSE3, MSE4, MSE5))

ggplot(error)+
    geom_point(aes(x=model_number, y=MSE_tr)) +
    geom_line(aes(x=model_number, y=MSE_tr)) +
    labs(x = 'Model Number', y = 'Training Error', title = 'Training Error for the Models')

error <- error %>% 
    add_column(MSE_tst=rep(0, 5))

for(i in 1:5){
    
    # grab the trained model
    model <- models[[i]]
    
    # get the predictions for the test data, compute the residuals
    
    test_results <- test %>% 
        mutate(G3_pred = predict(model, newdata=test)) %>% 
        mutate(resid_sq = (G3-G3_pred)^2) 
    
    # compute the MSE
    mst_tst <- summarise(test_results, mse_tst = mean(resid_sq))[[1]]
    
    error[i, 'MSE_tst'] <- mst_tst
}

error %>% 
    rename(tr=MSE_tr, tst=MSE_tst) %>% 
    gather(key=type, value=error, tr, tst) %>% 
    ggplot() +
    geom_point(aes(x=degree, y=log10(error), color=type)) +
    geom_line(aes(x=degree, y=log10(error), color=type)) + 
    labs(x = 'Model Number', y = 'log10(Error)', title = 'Comparison of Training and Test Errors')
error
```

